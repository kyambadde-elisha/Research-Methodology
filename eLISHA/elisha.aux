\relax 
\citation{Timothy}
\citation{Hoff}
\citation{Clark}
\bibcite{Timothy}{1}
\bibcite{Hoff}{2}
\@writefile{toc}{\contentsline {section}{\numberline {1}\sc  LITERATURE REVIEW ON SPANNER}{2}}
\@writefile{toc}{\contentsline {paragraph}{\sl  Google has proven time and again it is on the extreme bleeding edge of invention when it comes to scale out architectures that make supercomputers look like toys. But what would the world look like if the search engine giant had started selling capacity on its vast infrastructure back in 2005, before Amazon Web Services launched, and then shortly thereafter started selling capacity on its high level platform services? And what if it had open sourced these technologies, as it has done with the Kubernetes container controller? That, in a nutshell, is why it has taken nearly ten years since Google first started development of Spanner and five years from when Google released its paper on this globe-spanning, distributed SQL database to when it is available as a service on Google’s Cloud Platform public cloud, aimed at more generic workloads than its own AdWords and Google Play.}{2}}
\@writefile{toc}{\contentsline {paragraph}{ \sl  Google recently released a paper on Spanner, their planet enveloping tool for organizing the world’s monetizable information. Reading the Spanner paper I felt it had that chiseled in stone feel that all of Google’s best papers have. An instant classic. Jeff Dean foreshadowed Spanner’s humungousness as early as 2009. Now Spanner seems fully online, just waiting to handle “millions of machines across hundreds of datacenters and trillions of database rows.}{2}}
\@writefile{toc}{\contentsline {paragraph}{\sl  "Spanner is impressive work on one of the hardest distributed systems problems — a globally replicated database that supports externally consistent transactions within reasonable latency bounds," Andy Gross, principal architect at Basho, a company that makes the Riak distributed database, told ZDNet. Applications that use Spanner, such as Google's 'F1' advertising backend, can specify which datacentres contain which bits of data so that frequently read data can be located near users to reduce write latency. They can even specify how many datacentres store the data, to add as many layers of redundancy as there are Google datacentres. .}{2}}
\bibcite{Clark}{3}
